{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a21b4079",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\shara\\OneDrive\\Documents\\Live Courses Krish Naik\\R_A_G\\LLamaIndex\\.llama\\Lib\\site-packages\\llama_index\\embeddings\\gemini\\base.py:7: FutureWarning: \n",
      "\n",
      "All support for the `google.generativeai` package has ended. It will no longer be receiving \n",
      "updates or bug fixes. Please switch to the `google.genai` package as soon as possible.\n",
      "See README for more details:\n",
      "\n",
      "https://github.com/google-gemini/deprecated-generative-ai-python/blob/main/README.md\n",
      "\n",
      "  import google.generativeai as gemini\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Imports successful!\n"
     ]
    }
   ],
   "source": [
    "# Core LlamaIndex\n",
    "from llama_index.core import (\n",
    "    VectorStoreIndex,\n",
    "    SimpleDirectoryReader,\n",
    "    Settings,\n",
    "    Document,\n",
    "    StorageContext,\n",
    "    load_index_from_storage,\n",
    ")\n",
    "from llama_index.core.vector_stores import VectorStoreInfo, MetadataInfo\n",
    "from llama_index.core.retrievers import VectorIndexRetriever, VectorIndexAutoRetriever\n",
    "from llama_index.core.query_engine import RetrieverQueryEngine\n",
    "\n",
    "# Vector Stores\n",
    "from llama_index.vector_stores.chroma import ChromaVectorStore\n",
    "try:\n",
    "    from llama_index.vector_stores.qdrant import QdrantVectorStore\n",
    "    QDRANT_AVAILABLE = True\n",
    "except ImportError:\n",
    "    QDRANT_AVAILABLE = False\n",
    "    print(\"⚠️  Qdrant not installed. Install with: pip install llama-index-vector-stores-qdrant qdrant-client\")\n",
    "\n",
    "# Embeddings\n",
    "from llama_index.embeddings.gemini import GeminiEmbedding\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "\n",
    "# LLM\n",
    "from llama_index.llms.groq import Groq\n",
    "\n",
    "# External libraries\n",
    "import chromadb\n",
    "if QDRANT_AVAILABLE:\n",
    "    from qdrant_client import QdrantClient\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from pathlib import Path\n",
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"✅ Imports successful!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "66550022",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Settings configured\n"
     ]
    }
   ],
   "source": [
    "# Load environment and configure Settings\n",
    "load_dotenv()\n",
    "\n",
    "# Configure LLM\n",
    "Settings.llm = Groq(model=\"Qwen/Qwen3-32B\",temperature=0.1)\n",
    "\n",
    "# Configure Embedding Model\n",
    "Settings.embed_model = GeminiEmbedding(\n",
    "    model_name=\"models/gemini-embedding-001\",title=\"this is a document\",\n",
    ")\n",
    "\n",
    "Settings.chunk_size = 1024\n",
    "Settings.chunk_overlap = 200\n",
    "\n",
    "print(\"✅ Settings configured\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5f7d2120",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Created 5 sample documents\n",
      "   Topics: qdrant, chroma, vector_databases, embeddings, algorithms\n"
     ]
    }
   ],
   "source": [
    "# Create comprehensive sample documents\n",
    "documents = [\n",
    "    Document(\n",
    "        text=\"\"\"\n",
    "        Vector databases are specialized databases designed to store and query high-dimensional vectors.\n",
    "        These vectors typically represent embeddings of text, images, or other data. Vector databases\n",
    "        enable efficient similarity search using algorithms like HNSW (Hierarchical Navigable Small World)\n",
    "        or IVF (Inverted File Index). Popular vector databases include Qdrant, Pinecone, Weaviate, and Milvus.\n",
    "        \"\"\",\n",
    "        metadata={\"topic\": \"vector_databases\", \"difficulty\": \"intermediate\", \"year\": 2023}\n",
    "    ),\n",
    "    Document(\n",
    "        text=\"\"\"\n",
    "        HNSW (Hierarchical Navigable Small World) is a graph-based algorithm for approximate nearest neighbor\n",
    "        search. It builds a multi-layer graph where each layer is a subset of the previous one. The algorithm\n",
    "        achieves excellent query performance (sub-millisecond) with high recall. HNSW parameters include\n",
    "        M (number of connections per node) and ef_construction (search width during construction).\n",
    "        \"\"\",\n",
    "        metadata={\"topic\": \"algorithms\", \"difficulty\": \"advanced\", \"year\": 2023}\n",
    "    ),\n",
    "    Document(\n",
    "        text=\"\"\"\n",
    "        Embedding models convert text into dense vector representations that capture semantic meaning.\n",
    "        OpenAI's text-embedding-3-small produces 1536-dimensional vectors and is optimized for retrieval tasks.\n",
    "        Open-source alternatives include sentence-transformers models like all-MiniLM-L6-v2 (384 dimensions)\n",
    "        and all-mpnet-base-v2 (768 dimensions). The choice of embedding model affects retrieval quality and cost.\n",
    "        \"\"\",\n",
    "        metadata={\"topic\": \"embeddings\", \"difficulty\": \"beginner\", \"year\": 2024}\n",
    "    ),\n",
    "    Document(\n",
    "        text=\"\"\"\n",
    "        Qdrant is an open-source vector database written in Rust. It supports HNSW indexing, filtering,\n",
    "        and hybrid search. Qdrant can run locally (Docker) or in the cloud. Key features include payload\n",
    "        filtering, quantization for memory reduction, and distributed deployments. Qdrant is particularly\n",
    "        well-suited for production RAG applications.\n",
    "        \"\"\",\n",
    "        metadata={\"topic\": \"qdrant\", \"difficulty\": \"intermediate\", \"year\": 2024}\n",
    "    ),\n",
    "    Document(\n",
    "        text=\"\"\"\n",
    "        Chroma is a lightweight, embedded vector database designed for AI applications. It runs in-memory\n",
    "        or can persist to disk. Chroma is easy to set up and integrates seamlessly with LangChain and LlamaIndex.\n",
    "        It's ideal for prototyping and small-to-medium scale applications. Chroma supports metadata filtering\n",
    "        and multiple distance metrics (cosine, euclidean, dot product).\n",
    "        \"\"\",\n",
    "        metadata={\"topic\": \"chroma\", \"difficulty\": \"beginner\", \"year\": 2024}\n",
    "    ),\n",
    "]\n",
    "\n",
    "print(f\"✅ Created {len(documents)} sample documents\")\n",
    "print(f\"   Topics: {', '.join(set(d.metadata['topic'] for d in documents))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "496c6526",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating VectorStoreIndex (in-memory)...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca389de23a644263839c79d1b51a6ec5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Parsing nodes:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4abd5d575adc4d238e9cecb3392b09aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating embeddings:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Index created in 3.20 seconds\n",
      "   Vector store type: SimpleVectorStore (in-memory)\n"
     ]
    }
   ],
   "source": [
    "# Create index with default in-memory vector store\n",
    "print(\"Creating VectorStoreIndex (in-memory)...\")\n",
    "start_time = time.time()\n",
    "\n",
    "simple_index = VectorStoreIndex.from_documents(\n",
    "    documents,\n",
    "    show_progress=True,\n",
    ")\n",
    "\n",
    "elapsed = time.time() - start_time\n",
    "print(f\"\\n✅ Index created in {elapsed:.2f} seconds\")\n",
    "print(f\"   Vector store type: SimpleVectorStore (in-memory)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f8c1a3d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating VectorStoreIndex with Chroma...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "884c0b7c6f7a48d192cf69d5af5c77c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Parsing nodes:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d2795136bef947559cd095cb6dc1339b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating embeddings:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Chroma index created in 2.59 seconds\n",
      "   Collection: llama_index_docs\n",
      "   Documents indexed: 5\n"
     ]
    }
   ],
   "source": [
    "# Initialize Chroma client (in-memory)\n",
    "chroma_client = chromadb.EphemeralClient()  # In-memory\n",
    "# For persistence: chromadb.PersistentClient(path=\"./chroma_db\")\n",
    "\n",
    "# Create collection\n",
    "collection_name = \"llama_index_docs\"\n",
    "chroma_collection = chroma_client.create_collection(collection_name)\n",
    "\n",
    "# Create Chroma vector store\n",
    "chroma_vector_store = ChromaVectorStore(chroma_collection=chroma_collection)\n",
    "\n",
    "# Create storage context\n",
    "storage_context = StorageContext.from_defaults(vector_store=chroma_vector_store)\n",
    "\n",
    "print(\"Creating VectorStoreIndex with Chroma...\")\n",
    "start_time = time.time()\n",
    "\n",
    "chroma_index = VectorStoreIndex.from_documents(\n",
    "    documents,\n",
    "    storage_context=storage_context,\n",
    "    show_progress=True,\n",
    ")\n",
    "\n",
    "elapsed = time.time() - start_time\n",
    "print(f\"\\n✅ Chroma index created in {elapsed:.2f} seconds\")\n",
    "print(f\"   Collection: {collection_name}\")\n",
    "print(f\"   Documents indexed: {chroma_collection.count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cf084009",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating VectorStoreIndex with Qdrant...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "36561925934f4c01bdf2b5c7a5521909",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Parsing nodes:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "966ef49dacdb49be9bae55565bbda153",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating embeddings:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Qdrant index created in 3.37 seconds\n",
      "   Collection: llama_index_qdrant\n"
     ]
    }
   ],
   "source": [
    "if QDRANT_AVAILABLE:\n",
    "    # Initialize Qdrant client (in-memory)\n",
    "    qdrant_client = QdrantClient(location=\":memory:\")\n",
    "    # For persistence: QdrantClient(path=\"./qdrant_db\")\n",
    "    # For cloud: QdrantClient(url=os.getenv(\"QDRANT_URL\"), api_key=os.getenv(\"QDRANT_API_KEY\"))\n",
    "    \n",
    "    # Create Qdrant vector store\n",
    "    qdrant_vector_store = QdrantVectorStore(\n",
    "        client=qdrant_client,\n",
    "        collection_name=\"llama_index_qdrant\",\n",
    "    )\n",
    "    \n",
    "    # Create storage context\n",
    "    qdrant_storage_context = StorageContext.from_defaults(vector_store=qdrant_vector_store)\n",
    "    \n",
    "    print(\"Creating VectorStoreIndex with Qdrant...\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    qdrant_index = VectorStoreIndex.from_documents(\n",
    "        documents,\n",
    "        storage_context=qdrant_storage_context,\n",
    "        show_progress=True,\n",
    "    )\n",
    "    \n",
    "    elapsed = time.time() - start_time\n",
    "    print(f\"\\n✅ Qdrant index created in {elapsed:.2f} seconds\")\n",
    "    print(f\"   Collection: llama_index_qdrant\")\n",
    "else:\n",
    "    print(\"⚠️  Skipping Qdrant example (not installed)\")\n",
    "    qdrant_index = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6dfac283",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenAI Embedding (text-embedding-3-small):\n",
      "  Dimensions: 3072\n",
      "  Time: 834.91ms\n",
      "  First 5 values: [-0.0456277, -0.0052703377, 0.00029374406, -0.089979425, 0.0037476365]\n"
     ]
    }
   ],
   "source": [
    "# Test OpenAI embedding\n",
    "# openai_embed = OpenAIEmbedding(\n",
    "#     model=\"text-embedding-3-small\",\n",
    "#     dimensions=1536,\n",
    "# )\n",
    "\n",
    "gemini_embed = GeminiEmbedding(\n",
    "    model_name=\"models/gemini-embedding-001\"\n",
    ")\n",
    "\n",
    "test_text = \"Vector databases enable semantic search\"\n",
    "start_time = time.time()\n",
    "gemini_vector = gemini_embed.get_text_embedding(test_text)\n",
    "openai_time = time.time() - start_time\n",
    "\n",
    "print(f\"OpenAI Embedding (text-embedding-3-small):\")\n",
    "print(f\"  Dimensions: {len(gemini_vector)}\")\n",
    "print(f\"  Time: {openai_time*1000:.2f}ms\")\n",
    "print(f\"  First 5 values: {gemini_vector[:5]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "44377d41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading HuggingFace model (this may take a moment on first run)...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f7c1325e83e4193b8d8fbe7052d23ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b5a92c532d7a4364a9f954a82a9980d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b33c67c4b46c423697722696355ffa31",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa3d2acd29a24ac48bdbc7b970942d51",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "83197ca4291345bd81a343d5f7bc1a5f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/612 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "454eb561414142d6b972d918dabb714d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/90.9M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f5abe5edc3e14702867d4323e5dfe663",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/350 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d9797d556dbc44e7ad5c01a8203e4d30",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7edc8fa8cc3c41559348c4798a8d947d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2eb450fc053347f1bee1049c94297d4a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "976021bdd5d44aedbab471bcb92b72cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "HuggingFace Embedding (all-MiniLM-L6-v2):\n",
      "  Dimensions: 384\n",
      "  Time: 445.14ms\n",
      "  First 5 values: [0.04381895065307617, -0.00954702403396368, -0.020091237500309944, 0.015027557499706745, 0.013344950042665005]\n"
     ]
    }
   ],
   "source": [
    "# Test HuggingFace embedding\n",
    "print(\"Loading HuggingFace model (this may take a moment on first run)...\")\n",
    "hf_embed = HuggingFaceEmbedding(\n",
    "    model_name=\"sentence-transformers/all-MiniLM-L6-v2\",  # 384 dimensions\n",
    ")\n",
    "\n",
    "start_time = time.time()\n",
    "hf_vector = hf_embed.get_text_embedding(test_text)\n",
    "hf_time = time.time() - start_time\n",
    "\n",
    "print(f\"\\nHuggingFace Embedding (all-MiniLM-L6-v2):\")\n",
    "print(f\"  Dimensions: {len(hf_vector)}\")\n",
    "print(f\"  Time: {hf_time*1000:.2f}ms\")\n",
    "print(f\"  First 5 values: {hf_vector[:5]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "682c4d78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Embedding Model Comparison:\n",
      "           Model  Dimensions Time (ms)            Cost   Quality Hosting\n",
      " Groq embed_mode        3072    834.91 $0.02/1M tokens Excellent     API\n",
      "all-MiniLM-L6-v2         384    445.14            Free      Good   Local\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "comparison = pd.DataFrame([\n",
    "    {\n",
    "        \"Model\": \"Groq embed_mode\",\n",
    "        \"Dimensions\": len(gemini_vector),\n",
    "        \"Time (ms)\": f\"{openai_time*1000:.2f}\",\n",
    "        \"Cost\": \"$0.02/1M tokens\",\n",
    "        \"Quality\": \"Excellent\",\n",
    "        \"Hosting\": \"API\",\n",
    "    },\n",
    "    {\n",
    "        \"Model\": \"all-MiniLM-L6-v2\",\n",
    "        \"Dimensions\": len(hf_vector),\n",
    "        \"Time (ms)\": f\"{hf_time*1000:.2f}\",\n",
    "        \"Cost\": \"Free\",\n",
    "        \"Quality\": \"Good\",\n",
    "        \"Hosting\": \"Local\",\n",
    "    },\n",
    "])\n",
    "\n",
    "print(\"\\nEmbedding Model Comparison:\")\n",
    "print(comparison.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "951b8123",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Persisting index to ./storage...\n",
      "\n",
      "✅ Index persisted successfully!\n",
      "   Location: ./storage\n",
      "   Files created: 5\n",
      "     - default__vector_store.json\n",
      "     - docstore.json\n",
      "     - graph_store.json\n",
      "     - image__vector_store.json\n",
      "     - index_store.json\n"
     ]
    }
   ],
   "source": [
    "# Save index to disk\n",
    "persist_dir = \"./storage\"\n",
    "\n",
    "print(f\"Persisting index to {persist_dir}...\")\n",
    "simple_index.storage_context.persist(persist_dir=persist_dir)\n",
    "\n",
    "print(\"\\n✅ Index persisted successfully!\")\n",
    "print(f\"   Location: {persist_dir}\")\n",
    "\n",
    "# Check what was saved\n",
    "storage_path = Path(persist_dir)\n",
    "if storage_path.exists():\n",
    "    files = list(storage_path.glob(\"*\"))\n",
    "    print(f\"   Files created: {len(files)}\")\n",
    "    for f in files:\n",
    "        print(f\"     - {f.name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2dd3a781",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def groqLlmResponse(response):\n",
    "    return  re.sub(r\"<think>.*?</think>\", \"\", response, flags=re.DOTALL).strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "173f34db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading index from ./storage...\n",
      "✅ Index loaded successfully!\n",
      "\n",
      "Test query on loaded index:\n",
      "  Query: What is Qdrant?\n",
      "  Response: Qdrant is an open-source vector database developed in Rust, designed for efficient storage and querying of high-dimensional vectors. It supports advanced capabilities such as HNSW indexing for similarity search, payload filtering, and hybrid search methods. The system offers flexibility by allowing deployment on local environments via Docker or cloud-based setups. Additional features include quantization techniques to reduce memory usage and support for distributed architectures, making it a suitable choice for production-level RAG (Retrieval-Augmented Generation) applications.\n"
     ]
    }
   ],
   "source": [
    "# Load index from disk\n",
    "print(f\"Loading index from {persist_dir}...\")\n",
    "\n",
    "storage_context_loaded = StorageContext.from_defaults(persist_dir=persist_dir)\n",
    "loaded_index = load_index_from_storage(storage_context_loaded)\n",
    "\n",
    "print(\"✅ Index loaded successfully!\")\n",
    "\n",
    "# Test the loaded index\n",
    "test_query_engine = loaded_index.as_query_engine(similarity_top_k=2)\n",
    "test_response = test_query_engine.query(\"What is Qdrant?\")\n",
    "clearResponse = test_response.response\n",
    "print(f\"\\nTest query on loaded index:\")\n",
    "print(f\"  Query: What is Qdrant?\")\n",
    "print(f\"  Response: {groqLlmResponse(clearResponse)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b9b6fe30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: What are the main vector databases mentioned?\n",
      "\n",
      "Response:\n",
      "The main vector databases mentioned are Qdrant, Pinecone, Weaviate, and Milvus.\n",
      "\n",
      "================================================================================\n",
      "\n",
      "Sources used: 3\n"
     ]
    }
   ],
   "source": [
    "# Create query engine with configuration\n",
    "query_engine = chroma_index.as_query_engine(\n",
    "    similarity_top_k=3,\n",
    "    response_mode=\"compact\",\n",
    ")\n",
    "\n",
    "query = \"What are the main vector databases mentioned?\"\n",
    "response = query_engine.query(query)\n",
    "clear_res = response.response\n",
    "\n",
    "print(f\"Query: {query}\\n\")\n",
    "print(f\"Response:\\n{groqLlmResponse(clear_res)}\")\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(f\"\\nSources used: {len(response.source_nodes)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e603b4a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing response modes with query: 'Explain HNSW algorithm'\n",
      "\n",
      "================================================================================\n",
      "\n",
      "Mode: compact\n",
      "  Time: 2.23s\n",
      "  Response length: 2382 chars\n",
      "  Response: HNSW (Hierarchical Navigable Small World) is a graph-based algorithm designed for efficient approximate nearest neighbor searches in high-dimensional spaces. It organizes data points into a multi-laye...\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Mode: tree_summarize\n",
      "  Time: 1.92s\n",
      "  Response length: 2042 chars\n",
      "  Response: The HNSW (Hierarchical Navigable Small World) algorithm is designed for efficient approximate nearest neighbor search in high-dimensional spaces. It organizes data into a multi-layered graph structure...\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Mode: simple_summarize\n",
      "  Time: 3.38s\n",
      "  Response length: 3546 chars\n",
      "  Response: The HNSW (Hierarchical Navigable Small World) algorithm is designed for efficient approximate nearest neighbor search in high-dimensional spaces. It organizes data into a multi-layered graph structure...\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Mode: refine\n",
      "  Time: 5.04s\n",
      "  Response length: 4124 chars\n",
      "  Response: The HNSW (Hierarchical Navigable Small World) algorithm is a graph-based method optimized for similarity search in vector databases, which handle high-dimensional data like embeddings from text, image...\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Test all response modes\n",
    "modes = [\"compact\", \"tree_summarize\", \"simple_summarize\", \"refine\"]\n",
    "test_query = \"Explain HNSW algorithm\"\n",
    "\n",
    "print(f\"Testing response modes with query: '{test_query}'\\n\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for mode in modes:\n",
    "    engine = chroma_index.as_query_engine(\n",
    "        similarity_top_k=2,\n",
    "        response_mode=mode\n",
    "    )\n",
    "    \n",
    "    start = time.time()\n",
    "    resp = engine.query(test_query)\n",
    "    clear_resp = resp.response\n",
    "    elapsed = time.time() - start\n",
    "    \n",
    "    print(f\"\\nMode: {mode}\")\n",
    "    print(f\"  Time: {elapsed:.2f}s\")\n",
    "    print(f\"  Response length: {len(str(resp))} chars\")\n",
    "    print(f\"  Response: {str(groqLlmResponse(clear_resp))[:200]}...\")\n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fe542f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: What is the difference between Qdrant and Chroma?\n",
      "\n",
      "Streaming response:\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Qdrant and Chroma are both vector databases tailored for AI applications but differ in their design, capabilities, and use cases. \n",
      "\n",
      "**Chroma** emphasizes simplicity and ease of integration, making it ideal for prototyping and small-to-medium projects. It supports metadata filtering and multiple distance metrics (e.g., cosine, Euclidean, dot product) while offering seamless compatibility with tools like LangChain and LlamaIndex. Its lightweight nature allows it to operate in-memory or persist to disk, prioritizing developer convenience.\n",
      "\n",
      "**Qdrant**, in contrast, is built for production-grade applications requiring advanced performance and scalability. It leverages Rust for efficiency, supports HNSW indexing, hybrid search, and distributed deployments. Features like payload filtering, quantization for memory optimization, and cloud/local deployment flexibility make it suitable for large-scale, high-throughput scenarios such as production RAG systems. \n",
      "\n",
      "In summary, Chroma focuses on simplicity and rapid development, while Qdrant emphasizes robustness, scalability, and advanced operational capabilities.\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Create streaming query engine\n",
    "streaming_engine = chroma_index.as_query_engine(\n",
    "    similarity_top_k=2,\n",
    "    streaming=True,\n",
    ")\n",
    "\n",
    "query = \"What is the difference between Qdrant and Chroma?\"\n",
    "print(f\"Query: {query}\\n\")\n",
    "print(\"Streaming response:\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "response = streaming_engine.query(query)\n",
    "\n",
    "inside_think = False\n",
    "buffer = \"\"\n",
    "\n",
    "# Stream tokens   content within <think> and </think> tags will be removed\n",
    "for text in response.response_gen:\n",
    "    buffer += text\n",
    "\n",
    "    while True:\n",
    "        if not inside_think:\n",
    "            if \"<think>\" in buffer:\n",
    "                before, buffer = buffer.split(\"<think>\", 1)\n",
    "                if before:\n",
    "                    print(before, end=\"\", flush=True)\n",
    "                inside_think = True\n",
    "            else:\n",
    "                print(buffer, end=\"\", flush=True)\n",
    "                buffer = \"\"\n",
    "                break\n",
    "        else:\n",
    "            if \"</think>\" in buffer:\n",
    "                _, buffer = buffer.split(\"</think>\", 1)\n",
    "                inside_think = False\n",
    "            else:\n",
    "                buffer = \"\"\n",
    "                break\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6b17ee72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: vector database algorithms\n",
      "\n",
      "Retrieved 3 nodes:\n",
      "\n",
      "Node 1:\n",
      "  Score: 0.7894\n",
      "  Topic: vector_databases\n",
      "  Difficulty: intermediate\n",
      "  Text (first 150 chars): Vector databases are specialized databases designed to store and query high-dimensional vectors.\n",
      "        These vectors typically represent embeddings ...\n",
      "\n",
      "Node 2:\n",
      "  Score: 0.7228\n",
      "  Topic: algorithms\n",
      "  Difficulty: advanced\n",
      "  Text (first 150 chars): HNSW (Hierarchical Navigable Small World) is a graph-based algorithm for approximate nearest neighbor\n",
      "        search. It builds a multi-layer graph wh...\n",
      "\n",
      "Node 3:\n",
      "  Score: 0.6968\n",
      "  Topic: qdrant\n",
      "  Difficulty: intermediate\n",
      "  Text (first 150 chars): Qdrant is an open-source vector database written in Rust. It supports HNSW indexing, filtering,\n",
      "        and hybrid search. Qdrant can run locally (Doc...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create custom retriever\n",
    "retriever = VectorIndexRetriever(\n",
    "    index=chroma_index,\n",
    "    similarity_top_k=3,\n",
    ")\n",
    "\n",
    "# Retrieve nodes directly (no LLM synthesis)\n",
    "query_str = \"vector database algorithms\"\n",
    "retrieved_nodes = retriever.retrieve(query_str)\n",
    "\n",
    "print(f\"Query: {query_str}\\n\")\n",
    "print(f\"Retrieved {len(retrieved_nodes)} nodes:\\n\")\n",
    "\n",
    "for i, node in enumerate(retrieved_nodes, 1):\n",
    "    print(f\"Node {i}:\")\n",
    "    print(f\"  Score: {node.score:.4f}\")\n",
    "    print(f\"  Topic: {node.metadata.get('topic')}\")\n",
    "    print(f\"  Difficulty: {node.metadata.get('difficulty')}\")\n",
    "    print(f\"  Text (first 150 chars): {node.text[:150]}...\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "fc88c52f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response from custom retriever:\n",
      "The HNSW (Hierarchical Navigable Small World) algorithm is designed for efficient approximate nearest neighbor search in high-dimensional spaces. It organizes data points into a multi-layered graph structure, where each layer serves as a progressively sparser representation of the dataset. The topmost layer contains the fewest nodes, enabling rapid traversal, while lower layers retain more nodes for finer-grained searches. This hierarchical design allows queries to start at the top layer and navigate downward, narrowing the search scope with each layer.\n",
      "\n",
      "Key aspects of HNSW include:  \n",
      "1. **Graph Connectivity**: Each node connects to a fixed number of neighbors (controlled by the parameter *M*), balancing graph density and search efficiency.  \n",
      "2. **Construction Process**: During graph building, a search width parameter (*ef_construction*) determines how thoroughly candidate connections are explored, influencing both accuracy and construction time.  \n",
      "3. **Query Performance**: By leveraging the layered structure, HNSW achieves sub-millisecond search times while maintaining high recall, making it suitable for large-scale applications like vector databases.  \n",
      "\n",
      "The algorithm’s efficiency stems from its ability to reduce the effective search space exponentially with each layer, combining global exploration in upper layers with local refinement in lower layers.\n"
     ]
    }
   ],
   "source": [
    "# Build query engine from custom retriever\n",
    "custom_query_engine = RetrieverQueryEngine.from_args(\n",
    "    retriever=retriever,\n",
    "    response_mode=\"compact\",\n",
    ")\n",
    "\n",
    "response = custom_query_engine.query(\"Explain the HNSW algorithm\")\n",
    "clear_resp = response.response\n",
    "print(f\"Response from custom retriever:\\n{groqLlmResponse(clear_resp)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "dd42057e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ VectorIndexAutoRetriever configured\n"
     ]
    }
   ],
   "source": [
    "# Define metadata schema for auto-retriever\n",
    "vector_store_info = VectorStoreInfo(\n",
    "    content_info=\"Technical documentation about vector databases and embeddings\",\n",
    "    metadata_info=[\n",
    "        MetadataInfo(\n",
    "            name=\"topic\",\n",
    "            type=\"str\",\n",
    "            description=\"The main topic of the document (e.g., 'qdrant', 'chroma', 'embeddings')\",\n",
    "        ),\n",
    "        MetadataInfo(\n",
    "            name=\"difficulty\",\n",
    "            type=\"str\",\n",
    "            description=\"Difficulty level: 'beginner', 'intermediate', or 'advanced'\",\n",
    "        ),\n",
    "        MetadataInfo(\n",
    "            name=\"year\",\n",
    "            type=\"int\",\n",
    "            description=\"Year of publication (2023 or 2024)\",\n",
    "        ),\n",
    "    ],\n",
    ")\n",
    "\n",
    "# Create auto-retriever\n",
    "auto_retriever = VectorIndexAutoRetriever(\n",
    "    chroma_index,\n",
    "    vector_store_info=vector_store_info,\n",
    "    similarity_top_k=3,\n",
    ")\n",
    "\n",
    "print(\"✅ VectorIndexAutoRetriever configured\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "97f1006b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: Tell me about beginner-level topics\n",
      "\n",
      "Auto-retriever will automatically extract metadata filters from the query!\n",
      "\n",
      "Retrieved 2 nodes:\n",
      "\n",
      "Node 1:\n",
      "  Topic: embeddings\n",
      "  Difficulty: beginner\n",
      "  Score: 0.7903\n",
      "\n",
      "Node 2:\n",
      "  Topic: chroma\n",
      "  Difficulty: beginner\n",
      "  Score: 0.7271\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Query with natural language filters\n",
    "query_with_filter = \"Tell me about beginner-level topics\"\n",
    "\n",
    "print(f\"Query: {query_with_filter}\\n\")\n",
    "print(\"Auto-retriever will automatically extract metadata filters from the query!\\n\")\n",
    "\n",
    "retrieved = auto_retriever.retrieve(query_with_filter)\n",
    "\n",
    "print(f\"Retrieved {len(retrieved)} nodes:\\n\")\n",
    "for i, node in enumerate(retrieved, 1):\n",
    "    print(f\"Node {i}:\")\n",
    "    print(f\"  Topic: {node.metadata.get('topic')}\")\n",
    "    print(f\"  Difficulty: {node.metadata.get('difficulty')}\")\n",
    "    print(f\"  Score: {node.score:.4f}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4044be24",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".llama",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
