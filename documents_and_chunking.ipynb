{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7e59761d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Imports successful!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\shara\\OneDrive\\Documents\\Live Courses Krish Naik\\R_A_G\\LLamaIndex\\.llama\\Lib\\site-packages\\llama_index\\embeddings\\gemini\\base.py:7: FutureWarning: \n",
      "\n",
      "All support for the `google.generativeai` package has ended. It will no longer be receiving \n",
      "updates or bug fixes. Please switch to the `google.genai` package as soon as possible.\n",
      "See README for more details:\n",
      "\n",
      "https://github.com/google-gemini/deprecated-generative-ai-python/blob/main/README.md\n",
      "\n",
      "  import google.generativeai as gemini\n"
     ]
    }
   ],
   "source": [
    "# Core LlamaIndex\n",
    "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader, Settings, Document\n",
    "from llama_index.core.schema import TextNode, NodeRelationship, RelatedNodeInfo\n",
    "\n",
    "# Node Parsers (Chunking Strategies)\n",
    "from llama_index.core.node_parser import (\n",
    "    SentenceSplitter,\n",
    "    TokenTextSplitter,\n",
    "    SemanticSplitterNodeParser,\n",
    ")\n",
    "\n",
    "# Metadata Extraction\n",
    "from llama_index.core.extractors import (\n",
    "    TitleExtractor,\n",
    "    SummaryExtractor,\n",
    ")\n",
    "from llama_index.core.ingestion import IngestionPipeline\n",
    "\n",
    "# LLM and Embeddings\n",
    "from llama_index.llms.groq import Groq\n",
    "from llama_index.embeddings.gemini import GeminiEmbedding\n",
    "\n",
    "# Utilities\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"✅ Imports successful!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9b9d4b8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Settings configured\n"
     ]
    }
   ],
   "source": [
    "# Load environment variables and configure Settings\n",
    "load_dotenv()\n",
    "\n",
    "Settings.llm = Groq(model=\"Qwen/Qwen3-32B\",temperature=0.1)\n",
    "Settings.embed_model = GeminiEmbedding(\n",
    "    model_name=\"models/gemini-embedding-001\",title=\"this is a document\",\n",
    ")\n",
    "\n",
    "print(\"✅ Settings configured\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "11a20ac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def groqLlmResponse(response):\n",
    "    return  re.sub(r\"<think>.*?</think>\", \"\", response, flags=re.DOTALL).strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "06828731",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data directory exists: True\n",
      "Sample docs directory: True\n",
      "Research papers directory: True\n",
      "\n",
      "Found 1 PDF files in research_papers/\n",
      "  - VL_Jepa.pdf\n"
     ]
    }
   ],
   "source": [
    "# Check data directory structure\n",
    "data_dir = Path(\"./data\")\n",
    "sample_docs_dir = data_dir / \"sample_docs\"\n",
    "research_papers_dir = data_dir / \"research_papers\"\n",
    "\n",
    "print(f\"Data directory exists: {data_dir.exists()}\")\n",
    "print(f\"Sample docs directory: {sample_docs_dir.exists()}\")\n",
    "print(f\"Research papers directory: {research_papers_dir.exists()}\")\n",
    "\n",
    "if research_papers_dir.exists():\n",
    "    files = list(research_papers_dir.glob(\"*.pdf\"))\n",
    "    print(f\"\\nFound {len(files)} PDF files in research_papers/\")\n",
    "    for f in files:\n",
    "        print(f\"  - {f.name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "10c016e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Created 3 sample research papers\n",
      "  - Attention Is All You Need (2017)\n",
      "  - BERT (2019)\n",
      "  - RAG (2020)\n"
     ]
    }
   ],
   "source": [
    "# Create sample documents if no PDFs available\n",
    "# In practice, you'd load actual PDFs from the data directory\n",
    "\n",
    "sample_papers = [\n",
    "    Document(\n",
    "        text=\"\"\"\n",
    "        Title: Attention Is All You Need\n",
    "        Authors: Vaswani et al.\n",
    "        Year: 2017\n",
    "        \n",
    "        Abstract: The dominant sequence transduction models are based on complex recurrent or \n",
    "        convolutional neural networks that include an encoder and a decoder. The best performing \n",
    "        models also connect the encoder and decoder through an attention mechanism. We propose a \n",
    "        new simple network architecture, the Transformer, based solely on attention mechanisms, \n",
    "        dispensing with recurrence and convolutions entirely.\n",
    "        \n",
    "        Introduction: Recurrent neural networks, long short-term memory and gated recurrent neural \n",
    "        networks in particular, have been firmly established as state of the art approaches in \n",
    "        sequence modeling and transduction problems. The Transformer is the first transduction model \n",
    "        relying entirely on self-attention to compute representations of its input and output without \n",
    "        using sequence-aligned RNNs or convolution.\n",
    "        \"\"\",\n",
    "        metadata={\n",
    "            \"title\": \"Attention Is All You Need\",\n",
    "            \"authors\": \"Vaswani et al.\",\n",
    "            \"year\": 2017,\n",
    "            \"category\": \"transformers\",\n",
    "            \"citations\": 85000,\n",
    "            \"source\": \"research_paper\"\n",
    "        }\n",
    "    ),\n",
    "    Document(\n",
    "        text=\"\"\"\n",
    "        Title: BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\n",
    "        Authors: Devlin et al.\n",
    "        Year: 2019\n",
    "        \n",
    "        Abstract: We introduce a new language representation model called BERT, which stands for \n",
    "        Bidirectional Encoder Representations from Transformers. Unlike recent language representation \n",
    "        models, BERT is designed to pre-train deep bidirectional representations from unlabeled text \n",
    "        by jointly conditioning on both left and right context in all layers.\n",
    "        \n",
    "        Introduction: Language model pre-training has been shown to be effective for improving many \n",
    "        natural language processing tasks. Pre-trained language representations can be either context-free \n",
    "        or context-based. BERT alleviates the unidirectionality constraint by using a masked language \n",
    "        model (MLM) pre-training objective.\n",
    "        \"\"\",\n",
    "        metadata={\n",
    "            \"title\": \"BERT\",\n",
    "            \"authors\": \"Devlin et al.\",\n",
    "            \"year\": 2019,\n",
    "            \"category\": \"language_models\",\n",
    "            \"citations\": 65000,\n",
    "            \"source\": \"research_paper\"\n",
    "        }\n",
    "    ),\n",
    "    Document(\n",
    "        text=\"\"\"\n",
    "        Title: Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks\n",
    "        Authors: Lewis et al.\n",
    "        Year: 2020\n",
    "        \n",
    "        Abstract: Large pre-trained language models have been shown to store factual knowledge in their \n",
    "        parameters, and achieve state-of-the-art results when fine-tuned on downstream NLP tasks. However, \n",
    "        their ability to access and precisely manipulate knowledge is still limited. We explore a general \n",
    "        fine-tuning recipe for retrieval-augmented generation (RAG) models which combine parametric and \n",
    "        non-parametric memory.\n",
    "        \n",
    "        Introduction: Pre-trained neural language models store and retrieve knowledge using their parameters. \n",
    "        RAG models combine parametric memory (the LLM) with non-parametric memory (a dense vector index of \n",
    "        Wikipedia). This provides the model with access to up-to-date information and allows for more \n",
    "        interpretable and modular systems.\n",
    "        \"\"\",\n",
    "        metadata={\n",
    "            \"title\": \"RAG\",\n",
    "            \"authors\": \"Lewis et al.\",\n",
    "            \"year\": 2020,\n",
    "            \"category\": \"rag\",\n",
    "            \"citations\": 3500,\n",
    "            \"source\": \"research_paper\"\n",
    "        }\n",
    "    ),\n",
    "]\n",
    "\n",
    "print(f\"✅ Created {len(sample_papers)} sample research papers\")\n",
    "for doc in sample_papers:\n",
    "    print(f\"  - {doc.metadata['title']} ({doc.metadata['year']})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ab0a04ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enhanced metadata for first document:\n",
      "  title: Attention Is All You Need\n",
      "  authors: Vaswani et al.\n",
      "  year: 2017\n",
      "  category: transformers\n",
      "  citations: 85000\n",
      "  source: research_paper\n",
      "  processed_date: 2026-01-07T21:42:48.468343\n",
      "  char_count: 1006\n",
      "  word_count: 123\n"
     ]
    }
   ],
   "source": [
    "# Add processing metadata\n",
    "for doc in sample_papers:\n",
    "    doc.metadata[\"processed_date\"] = datetime.now().isoformat()\n",
    "    doc.metadata[\"char_count\"] = len(doc.text)\n",
    "    doc.metadata[\"word_count\"] = len(doc.text.split())\n",
    "\n",
    "print(\"Enhanced metadata for first document:\")\n",
    "for key, value in sample_papers[0].metadata.items():\n",
    "    print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1cbc2d40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SentenceSplitter Results:\n",
      "  Input documents: 3\n",
      "  Output nodes: 3\n",
      "  Avg chars per node: 936\n",
      "\n",
      "First node preview:\n",
      "  Text (first 200 chars): Title: Attention Is All You Need\n",
      "        Authors: Vaswani et al.\n",
      "        Year: 2017\n",
      "\n",
      "        Abstract: The dominant sequence transduction models are based on complex recurrent or \n",
      "        convolutiona...\n",
      "  Metadata: {'title': 'Attention Is All You Need', 'authors': 'Vaswani et al.', 'year': 2017, 'category': 'transformers', 'citations': 85000, 'source': 'research_paper', 'processed_date': '2026-01-07T21:42:48.468343', 'char_count': 1006, 'word_count': 123}\n"
     ]
    }
   ],
   "source": [
    "# SentenceSplitter: Respects sentence boundaries\n",
    "sentence_splitter = SentenceSplitter(\n",
    "    chunk_size=1024,     # Target tokens per chunk\n",
    "    chunk_overlap=200,   # Overlap to preserve context\n",
    "    separator=\" \",       # Split on spaces first\n",
    ")\n",
    "\n",
    "sentence_nodes = sentence_splitter.get_nodes_from_documents(sample_papers)\n",
    "\n",
    "print(f\"SentenceSplitter Results:\")\n",
    "print(f\"  Input documents: {len(sample_papers)}\")\n",
    "print(f\"  Output nodes: {len(sentence_nodes)}\")\n",
    "print(f\"  Avg chars per node: {sum(len(n.text) for n in sentence_nodes) / len(sentence_nodes):.0f}\")\n",
    "\n",
    "print(f\"\\nFirst node preview:\")\n",
    "print(f\"  Text (first 200 chars): {sentence_nodes[0].text[:200]}...\")\n",
    "print(f\"  Metadata: {sentence_nodes[0].metadata}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9ca6bae2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TokenTextSplitter Results:\n",
      "  Input documents: 3\n",
      "  Output nodes: 3\n",
      "  Avg chars per node: 936\n",
      "\n",
      "Comparison:\n",
      "  SentenceSplitter: 3 nodes\n",
      "  TokenTextSplitter: 3 nodes\n",
      "  Difference: 0 nodes\n"
     ]
    }
   ],
   "source": [
    "# TokenTextSplitter: Precise token count control\n",
    "token_splitter = TokenTextSplitter(\n",
    "    chunk_size=512,      # Exact token limit\n",
    "    chunk_overlap=128,   # 25% overlap\n",
    "    separator=\" \",\n",
    ")\n",
    "\n",
    "token_nodes = token_splitter.get_nodes_from_documents(sample_papers)\n",
    "\n",
    "print(f\"TokenTextSplitter Results:\")\n",
    "print(f\"  Input documents: {len(sample_papers)}\")\n",
    "print(f\"  Output nodes: {len(token_nodes)}\")\n",
    "print(f\"  Avg chars per node: {sum(len(n.text) for n in token_nodes) / len(token_nodes):.0f}\")\n",
    "\n",
    "# Compare with sentence splitter\n",
    "print(f\"\\nComparison:\")\n",
    "print(f\"  SentenceSplitter: {len(sentence_nodes)} nodes\")\n",
    "print(f\"  TokenTextSplitter: {len(token_nodes)} nodes\")\n",
    "print(f\"  Difference: {abs(len(sentence_nodes) - len(token_nodes))} nodes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bc5791a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "First node preview:\n",
      "  Text (first 200 chars): Title: BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\n",
      "        Authors: Devlin et al.\n",
      "        Year: 2019\n",
      "\n",
      "        Abstract: We introduce a new language representation ...\n",
      "  Metadata: {'title': 'BERT', 'authors': 'Devlin et al.', 'year': 2019, 'category': 'language_models', 'citations': 65000, 'source': 'research_paper', 'processed_date': '2026-01-07T21:42:48.472254', 'char_count': 895, 'word_count': 102}\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\nFirst node preview:\")\n",
    "print(f\"  Text (first 200 chars): {token_nodes[1].text[:200]}...\")\n",
    "print(f\"  Metadata: {token_nodes[1].metadata}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e9ed26af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating semantic chunks (this will call embedding API)...\n",
      "\n",
      "SemanticSplitterNodeParser Results:\n",
      "  Input documents: 3\n",
      "  Output nodes: 6\n",
      "  Avg chars per node: 477\n",
      "  Min chars: 220\n",
      "  Max chars: 740\n"
     ]
    }
   ],
   "source": [
    "# SemanticSplitterNodeParser: Chunk by meaning, not just size\n",
    "semantic_splitter = SemanticSplitterNodeParser(\n",
    "    buffer_size=1,              # Sentences to group for comparison\n",
    "    breakpoint_percentile_threshold=95,  # Sensitivity to semantic breaks\n",
    "    embed_model=Settings.embed_model,\n",
    ")\n",
    "\n",
    "print(\"Creating semantic chunks (this will call embedding API)...\")\n",
    "semantic_nodes = semantic_splitter.get_nodes_from_documents(sample_papers)\n",
    "\n",
    "print(f\"\\nSemanticSplitterNodeParser Results:\")\n",
    "print(f\"  Input documents: {len(sample_papers)}\")\n",
    "print(f\"  Output nodes: {len(semantic_nodes)}\")\n",
    "print(f\"  Avg chars per node: {sum(len(n.text) for n in semantic_nodes) / len(semantic_nodes):.0f}\")\n",
    "print(f\"  Min chars: {min(len(n.text) for n in semantic_nodes)}\")\n",
    "print(f\"  Max chars: {max(len(n.text) for n in semantic_nodes)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7769a731",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "First node preview:\n",
      "  Text (first 200 chars): The best performing \n",
      "        models also connect the encoder and decoder through an attention mechanism. We propose a \n",
      "        new simple network architecture, the Transformer, based solely on attenti...\n",
      "  Metadata: {'title': 'Attention Is All You Need', 'authors': 'Vaswani et al.', 'year': 2017, 'category': 'transformers', 'citations': 85000, 'source': 'research_paper', 'processed_date': '2026-01-07T21:42:48.468343', 'char_count': 1006, 'word_count': 123}\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\nFirst node preview:\")\n",
    "print(f\"  Text (first 200 chars): {semantic_nodes[1].text[:200]}...\")\n",
    "print(f\"  Metadata: {semantic_nodes[1].metadata}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4fc959ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Chunking Strategy Comparison:\n",
      "Strategy  Num Nodes  Avg Chars  Min Chars  Max Chars  Std Dev\n",
      "Sentence          3        936        877        988       55\n",
      "   Token          3        936        877        988       55\n",
      "Semantic          6        477        220        740      228\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Compare chunking strategies\n",
    "strategies = [\n",
    "    {\"name\": \"Sentence\", \"nodes\": sentence_nodes},\n",
    "    {\"name\": \"Token\", \"nodes\": token_nodes},\n",
    "    {\"name\": \"Semantic\", \"nodes\": semantic_nodes},\n",
    "]\n",
    "\n",
    "comparison_data = []\n",
    "for strat in strategies:\n",
    "    nodes = strat[\"nodes\"]\n",
    "    comparison_data.append({\n",
    "        \"Strategy\": strat[\"name\"],\n",
    "        \"Num Nodes\": len(nodes),\n",
    "        \"Avg Chars\": int(sum(len(n.text) for n in nodes) / len(nodes)),\n",
    "        \"Min Chars\": min(len(n.text) for n in nodes),\n",
    "        \"Max Chars\": max(len(n.text) for n in nodes),\n",
    "        \"Std Dev\": int(pd.Series([len(n.text) for n in nodes]).std()),\n",
    "    })\n",
    "\n",
    "df = pd.DataFrame(comparison_data)\n",
    "print(\"\\nChunking Strategy Comparison:\")\n",
    "print(df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "14e2974f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enhanced node metadata example:\n",
      "Node 0 metadata: {'title': 'Attention Is All You Need', 'authors': 'Vaswani et al.', 'year': 2017, 'category': 'transformers', 'citations': 85000, 'source': 'research_paper', 'processed_date': '2026-01-07T21:42:48.468343', 'char_count': 1006, 'word_count': 123, 'node_index': 0, 'chunk_strategy': 'sentence', 'has_abstract': True, 'has_introduction': True, 'mentions_transformer': True}\n"
     ]
    }
   ],
   "source": [
    "# Enrich nodes with custom metadata\n",
    "for i, node in enumerate(sentence_nodes):\n",
    "    # Add node-specific metadata\n",
    "    node.metadata[\"node_index\"] = i\n",
    "    node.metadata[\"chunk_strategy\"] = \"sentence\"\n",
    "    \n",
    "    # Derive metadata from content\n",
    "    text_lower = node.text.lower()\n",
    "    node.metadata[\"has_abstract\"] = \"abstract\" in text_lower\n",
    "    node.metadata[\"has_introduction\"] = \"introduction\" in text_lower\n",
    "    node.metadata[\"mentions_transformer\"] = \"transformer\" in text_lower\n",
    "\n",
    "print(\"Enhanced node metadata example:\")\n",
    "print(f\"Node 0 metadata: {sentence_nodes[0].metadata}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "752822e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[TextNode(id_='4a0dce17-ef81-4299-baf8-59beb11cd73f', embedding=None, metadata={'title': 'Attention Is All You Need', 'authors': 'Vaswani et al.', 'year': 2017, 'category': 'transformers', 'citations': 85000, 'source': 'research_paper', 'processed_date': '2026-01-07T21:42:48.468343', 'char_count': 1006, 'word_count': 123, 'node_index': 0, 'chunk_strategy': 'sentence', 'has_abstract': True, 'has_introduction': True, 'mentions_transformer': True}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='329dffd5-441c-46bc-930d-8d1c742f8383', node_type=<ObjectType.DOCUMENT: '4'>, metadata={'title': 'Attention Is All You Need', 'authors': 'Vaswani et al.', 'year': 2017, 'category': 'transformers', 'citations': 85000, 'source': 'research_paper', 'processed_date': '2026-01-07T21:42:48.468343', 'char_count': 1006, 'word_count': 123}, hash='ae44231a79eb163ab22855889ec29bd4f39427604a63ba770b47fa5512fa91e0')}, metadata_template='{key}: {value}', metadata_separator='\\n', text='Title: Attention Is All You Need\\n        Authors: Vaswani et al.\\n        Year: 2017\\n\\n        Abstract: The dominant sequence transduction models are based on complex recurrent or \\n        convolutional neural networks that include an encoder and a decoder. The best performing \\n        models also connect the encoder and decoder through an attention mechanism. We propose a \\n        new simple network architecture, the Transformer, based solely on attention mechanisms, \\n        dispensing with recurrence and convolutions entirely.\\n\\n        Introduction: Recurrent neural networks, long short-term memory and gated recurrent neural \\n        networks in particular, have been firmly established as state of the art approaches in \\n        sequence modeling and transduction problems. The Transformer is the first transduction model \\n        relying entirely on self-attention to compute representations of its input and output without \\n        using sequence-aligned RNNs or convolution.', mimetype='text/plain', start_char_idx=9, end_char_idx=997, metadata_seperator='\\n', text_template='{metadata_str}\\n\\n{content}'),\n",
       " TextNode(id_='7bdafe77-1128-45ac-b157-6e60f3f2f709', embedding=None, metadata={'title': 'BERT', 'authors': 'Devlin et al.', 'year': 2019, 'category': 'language_models', 'citations': 65000, 'source': 'research_paper', 'processed_date': '2026-01-07T21:42:48.472254', 'char_count': 895, 'word_count': 102, 'node_index': 1, 'chunk_strategy': 'sentence', 'has_abstract': True, 'has_introduction': True, 'mentions_transformer': True}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='1b080091-10da-4a0a-9f6d-3be2a7d265e7', node_type=<ObjectType.DOCUMENT: '4'>, metadata={'title': 'BERT', 'authors': 'Devlin et al.', 'year': 2019, 'category': 'language_models', 'citations': 65000, 'source': 'research_paper', 'processed_date': '2026-01-07T21:42:48.472254', 'char_count': 895, 'word_count': 102}, hash='28d5dc59e643ea842f25ce7a9d4d44343eee99e3cd6d4e610ead27d59c1d7838')}, metadata_template='{key}: {value}', metadata_separator='\\n', text='Title: BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\\n        Authors: Devlin et al.\\n        Year: 2019\\n\\n        Abstract: We introduce a new language representation model called BERT, which stands for \\n        Bidirectional Encoder Representations from Transformers. Unlike recent language representation \\n        models, BERT is designed to pre-train deep bidirectional representations from unlabeled text \\n        by jointly conditioning on both left and right context in all layers.\\n\\n        Introduction: Language model pre-training has been shown to be effective for improving many \\n        natural language processing tasks. Pre-trained language representations can be either context-free \\n        or context-based. BERT alleviates the unidirectionality constraint by using a masked language \\n        model (MLM) pre-training objective.', mimetype='text/plain', start_char_idx=9, end_char_idx=886, metadata_seperator='\\n', text_template='{metadata_str}\\n\\n{content}'),\n",
       " TextNode(id_='df4f8c21-537c-4814-b870-fa44107f41dd', embedding=None, metadata={'title': 'RAG', 'authors': 'Lewis et al.', 'year': 2020, 'category': 'rag', 'citations': 3500, 'source': 'research_paper', 'processed_date': '2026-01-07T21:42:48.473766', 'char_count': 961, 'word_count': 112, 'node_index': 2, 'chunk_strategy': 'sentence', 'has_abstract': True, 'has_introduction': True, 'mentions_transformer': False}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='05807c2d-40da-4d34-95e4-056698f26a1d', node_type=<ObjectType.DOCUMENT: '4'>, metadata={'title': 'RAG', 'authors': 'Lewis et al.', 'year': 2020, 'category': 'rag', 'citations': 3500, 'source': 'research_paper', 'processed_date': '2026-01-07T21:42:48.473766', 'char_count': 961, 'word_count': 112}, hash='118ad5640d4fe75a9e284579c2d218c549bbfdd615f9f53d88df8902a608aa03')}, metadata_template='{key}: {value}', metadata_separator='\\n', text='Title: Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks\\n        Authors: Lewis et al.\\n        Year: 2020\\n\\n        Abstract: Large pre-trained language models have been shown to store factual knowledge in their \\n        parameters, and achieve state-of-the-art results when fine-tuned on downstream NLP tasks. However, \\n        their ability to access and precisely manipulate knowledge is still limited. We explore a general \\n        fine-tuning recipe for retrieval-augmented generation (RAG) models which combine parametric and \\n        non-parametric memory.\\n\\n        Introduction: Pre-trained neural language models store and retrieve knowledge using their parameters. \\n        RAG models combine parametric memory (the LLM) with non-parametric memory (a dense vector index of \\n        Wikipedia). This provides the model with access to up-to-date information and allows for more \\n        interpretable and modular systems.', mimetype='text/plain', start_char_idx=9, end_char_idx=952, metadata_seperator='\\n', text_template='{metadata_str}\\n\\n{content}')]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7972a2be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting metadata with LLM (this may take a moment)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:01<00:00,  1.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Extracted summaries for 2 nodes\n",
      "\n",
      "Node 0 with LLM-generated summary:\n",
      "  Original text (first 150 chars): Title: Attention Is All You Need\n",
      "        Authors: Vaswani et al.\n",
      "        Year: 2017\n",
      "\n",
      "        Abstract: The dominant sequence transduction models are b...\n",
      "  Summary: **Summary:**  \n",
      "The section highlights the 2017 paper *\"Attention Is All You Need\"* by Vaswani et al., which introduced the **Transformer** architecture. Key topics include:  \n",
      "1. **Transformer Innovation**: A model that replaces traditional recurrent (RNNs, LSTMs, GRUs) and convolutional networks (CNNs) with **self-attention mechanisms**, enabling direct computation of input/output representations without sequential processing.  \n",
      "2. **Impact on NLP**: The Transformer became a foundational architecture in natural language processing (NLP), revolutionizing tasks like machine translation and text generation.  \n",
      "3. **Key Concepts**: Focus on *self-attention* for capturing dependencies, eliminating recurrence/convolution, and simplifying encoder-decoder frameworks.  \n",
      "\n",
      "**Entities**:  \n",
      "- **Authors**: Vaswani et al.  \n",
      "- **Year**: 2017  \n",
      "- **Models**: Transformer, RNNs, LSTMs, GRUs, CNNs  \n",
      "- **Concepts**: Self-attention, sequence transduction, encoder-decoder architecture.  \n",
      "\n",
      "The paper’s high citation count (85,000+) underscores its transformative role in modern AI.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Use LLM to extract metadata\n",
    "from llama_index.core.extractors import SummaryExtractor, TitleExtractor\n",
    "\n",
    "# Create extractors\n",
    "title_extractor = TitleExtractor(\n",
    "    llm=Settings.llm,\n",
    "    nodes=5,  # Look at first 5 nodes for title\n",
    ")\n",
    "\n",
    "summary_extractor = SummaryExtractor(\n",
    "    llm=Settings.llm,\n",
    "    summaries=[\"self\"],  # Summarize each node\n",
    ")\n",
    "\n",
    "print(\"Extracting metadata with LLM (this may take a moment)...\")\n",
    "\n",
    "# Apply to a subset of nodes (to save API calls)\n",
    "sample_nodes_for_extraction = sentence_nodes[:2]\n",
    "\n",
    "# Extract summaries\n",
    "nodes_with_summaries = summary_extractor.process_nodes(sample_nodes_for_extraction)\n",
    "\n",
    "print(f\"\\n✅ Extracted summaries for {len(nodes_with_summaries)} nodes\")\n",
    "print(f\"\\nNode 0 with LLM-generated summary:\")\n",
    "print(f\"  Original text (first 150 chars): {nodes_with_summaries[0].text[:150]}...\")\n",
    "if \"section_summary\" in nodes_with_summaries[0].metadata:\n",
    "    print(f\"  Summary: {groqLlmResponse(nodes_with_summaries[0].metadata['section_summary'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "03be093d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'title': 'Attention Is All You Need',\n",
       " 'authors': 'Vaswani et al.',\n",
       " 'year': 2017,\n",
       " 'category': 'transformers',\n",
       " 'citations': 85000,\n",
       " 'source': 'research_paper',\n",
       " 'processed_date': '2026-01-07T21:42:48.468343',\n",
       " 'char_count': 1006,\n",
       " 'word_count': 123,\n",
       " 'node_index': 0,\n",
       " 'chunk_strategy': 'sentence',\n",
       " 'has_abstract': True,\n",
       " 'has_introduction': True,\n",
       " 'mentions_transformer': True,\n",
       " 'section_summary': '<think>\\nOkay, let\\'s see. The user wants a summary of the key topics and entities from the provided section about the \"Attention Is All You Need\" paper. First, I need to parse the content given. The title is \"Attention Is All You Need\" by Vaswani et al. from 2017. The category is transformers, and it\\'s a research paper with a lot of citations. The abstract mentions that traditional models use recurrent or convolutional networks with encoder-decoder structures and attention mechanisms. The Transformer is introduced as a new architecture that uses only attention, removing recurrence and convolutions. The introduction contrasts RNNs, LSTMs, GRUs with the Transformer\\'s self-attention approach.\\n\\nKey topics here are the Transformer model, attention mechanisms, and how it replaces RNNs and CNNs. Entities include the authors, the paper\\'s title, year, and specific models like LSTM and GRU. Also, the significance of the paper is its introduction of the Transformer, which became foundational in NLP.\\n\\nI need to make sure the summary highlights the main contribution: the Transformer\\'s use of self-attention without recurrence or convolutions. Mention the comparison with existing models and the impact of the paper. Also, note the high citation count to emphasize its influence. Avoid technical jargon but keep it concise. Check if all important elements are covered: authors, year, key concepts, models compared, and the paper\\'s impact.\\n</think>\\n\\n**Summary:**  \\nThe section highlights the 2017 paper *\"Attention Is All You Need\"* by Vaswani et al., which introduced the **Transformer** architecture. Key topics include:  \\n1. **Transformer Innovation**: A model that replaces traditional recurrent (RNNs, LSTMs, GRUs) and convolutional networks (CNNs) with **self-attention mechanisms**, enabling direct computation of input/output representations without sequential processing.  \\n2. **Impact on NLP**: The Transformer became a foundational architecture in natural language processing (NLP), revolutionizing tasks like machine translation and text generation.  \\n3. **Key Concepts**: Focus on *self-attention* for capturing dependencies, eliminating recurrence/convolution, and simplifying encoder-decoder frameworks.  \\n\\n**Entities**:  \\n- **Authors**: Vaswani et al.  \\n- **Year**: 2017  \\n- **Models**: Transformer, RNNs, LSTMs, GRUs, CNNs  \\n- **Concepts**: Self-attention, sequence transduction, encoder-decoder architecture.  \\n\\nThe paper’s high citation count (85,000+) underscores its transformative role in modern AI.'}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nodes_with_summaries[0].metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e7413350",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:06<00:00,  3.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Extracted summaries for 2 nodes\n",
      "\n",
      "Node 0 with LLM-generated title:\n",
      "  Original text (first 150 chars): Title: Attention Is All You Need\n",
      "        Authors: Vaswani et al.\n",
      "        Year: 2017\n",
      "\n",
      "        Abstract: The dominant sequence transduction models are b...\n",
      "  Extracted title: Attention Is All You Need\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Extract summaries\n",
    "nodes_with_summaries = title_extractor.process_nodes(sample_nodes_for_extraction)\n",
    "\n",
    "print(f\"\\n✅ Extracted summaries for {len(nodes_with_summaries)} nodes\")\n",
    "print(f\"\\nNode 0 with LLM-generated title:\")\n",
    "print(f\"  Original text (first 150 chars): {nodes_with_summaries[0].text[:150]}...\")\n",
    "print(f\"  Extracted title: {nodes_with_summaries[0].metadata['title']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5e00f295",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Node Relationships:\n",
      "\n",
      "Node 0:\n",
      "  ID: 4a0dce17-ef81-4299-baf8-59beb11cd73f\n",
      "  Relationships: [<NodeRelationship.SOURCE: '1'>]\n",
      "  Source Document ID: 329dffd5-441c-46bc-930d-8d1c742f8383\n",
      "\n",
      "Node 1:\n",
      "  ID: 7bdafe77-1128-45ac-b157-6e60f3f2f709\n",
      "  Relationships: [<NodeRelationship.SOURCE: '1'>]\n",
      "  Source Document ID: 1b080091-10da-4a0a-9f6d-3be2a7d265e7\n",
      "\n",
      "Node 2:\n",
      "  ID: df4f8c21-537c-4814-b870-fa44107f41dd\n",
      "  Relationships: [<NodeRelationship.SOURCE: '1'>]\n",
      "  Source Document ID: 05807c2d-40da-4d34-95e4-056698f26a1d\n"
     ]
    }
   ],
   "source": [
    "# Inspect node relationships\n",
    "print(\"Node Relationships:\")\n",
    "for i, node in enumerate(sentence_nodes[:3]):\n",
    "    print(f\"\\nNode {i}:\")\n",
    "    print(f\"  ID: {node.node_id}\")\n",
    "    print(f\"  Relationships: {list(node.relationships.keys())}\")\n",
    "    \n",
    "    # Check for source document\n",
    "    if NodeRelationship.SOURCE in node.relationships:\n",
    "        source_info = node.relationships[NodeRelationship.SOURCE]\n",
    "        print(f\"  Source Document ID: {source_info.node_id}\")\n",
    "    \n",
    "    # Check for previous/next nodes\n",
    "    if NodeRelationship.PREVIOUS in node.relationships:\n",
    "        print(f\"  Has PREVIOUS node\")\n",
    "    if NodeRelationship.NEXT in node.relationships:\n",
    "        print(f\"  Has NEXT node\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "224e39e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Node Relationships:\n",
      "\n",
      "Node 0:\n",
      "  ID: b015e802-8907-4f87-b314-72199ece202e\n",
      "  Relationships: [<NodeRelationship.SOURCE: '1'>, <NodeRelationship.NEXT: '3'>]\n",
      "  Source Document ID: 329dffd5-441c-46bc-930d-8d1c742f8383\n",
      "  Has NEXT node\n",
      "\n",
      "Node 1:\n",
      "  ID: 11b6da02-2629-46a1-bf65-fb2f763de60f\n",
      "  Relationships: [<NodeRelationship.SOURCE: '1'>, <NodeRelationship.PREVIOUS: '2'>]\n",
      "  Source Document ID: 329dffd5-441c-46bc-930d-8d1c742f8383\n",
      "  Has PREVIOUS node\n",
      "\n",
      "Node 2:\n",
      "  ID: 25630aa5-c2bd-4d99-b204-6027ba6d9379\n",
      "  Relationships: [<NodeRelationship.SOURCE: '1'>, <NodeRelationship.NEXT: '3'>]\n",
      "  Source Document ID: 1b080091-10da-4a0a-9f6d-3be2a7d265e7\n",
      "  Has NEXT node\n",
      "\n",
      "Node 3:\n",
      "  ID: b894074a-e601-49fc-b852-c3e578a2a198\n",
      "  Relationships: [<NodeRelationship.SOURCE: '1'>, <NodeRelationship.PREVIOUS: '2'>]\n",
      "  Source Document ID: 1b080091-10da-4a0a-9f6d-3be2a7d265e7\n",
      "  Has PREVIOUS node\n",
      "\n",
      "Node 4:\n",
      "  ID: e7850622-3958-41a8-a402-dc4936960e44\n",
      "  Relationships: [<NodeRelationship.SOURCE: '1'>, <NodeRelationship.NEXT: '3'>]\n",
      "  Source Document ID: 05807c2d-40da-4d34-95e4-056698f26a1d\n",
      "  Has NEXT node\n",
      "\n",
      "Node 5:\n",
      "  ID: 1dd66156-5338-4b20-a849-8f26721e52ef\n",
      "  Relationships: [<NodeRelationship.SOURCE: '1'>, <NodeRelationship.PREVIOUS: '2'>]\n",
      "  Source Document ID: 05807c2d-40da-4d34-95e4-056698f26a1d\n",
      "  Has PREVIOUS node\n"
     ]
    }
   ],
   "source": [
    "# Inspect node relationships\n",
    "print(\"Node Relationships:\")\n",
    "for i, node in enumerate(semantic_nodes[:]):\n",
    "    print(f\"\\nNode {i}:\")\n",
    "    print(f\"  ID: {node.node_id}\")\n",
    "    print(f\"  Relationships: {list(node.relationships.keys())}\")\n",
    "    \n",
    "    # Check for source document\n",
    "    if NodeRelationship.SOURCE in node.relationships:\n",
    "        source_info = node.relationships[NodeRelationship.SOURCE]\n",
    "        print(f\"  Source Document ID: {source_info.node_id}\")\n",
    "    \n",
    "    # Check for previous/next nodes\n",
    "    if NodeRelationship.PREVIOUS in node.relationships:\n",
    "        print(f\"  Has PREVIOUS node\")\n",
    "    if NodeRelationship.NEXT in node.relationships:\n",
    "        print(f\"  Has NEXT node\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "295d33cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created hierarchical relationship:\n",
      "  Summary Node ID: f453e00d-fb00-4898-8abe-76d19ff17898\n",
      "  Child nodes: 3\n"
     ]
    }
   ],
   "source": [
    "# Create custom parent-child relationships\n",
    "# Example: Create a summary node that links to detail nodes\n",
    "\n",
    "summary_node = TextNode(\n",
    "    text=\"Summary: Research papers on transformers, BERT, and RAG\",\n",
    "    metadata={\"type\": \"summary\", \"level\": \"0\"},\n",
    ")\n",
    "\n",
    "# Link detail nodes as children\n",
    "for node in sentence_nodes[:3]:\n",
    "    node.relationships[NodeRelationship.PARENT] = RelatedNodeInfo(node_id=summary_node.node_id,)\n",
    "    node.metadata[\"level\"] = \"1\"\n",
    "\n",
    "print(\"Created hierarchical relationship:\")\n",
    "print(f\"  Summary Node ID: {summary_node.node_id}\")\n",
    "print(f\"  Child nodes: {len([n for n in sentence_nodes[:3] if NodeRelationship.PARENT in n.relationships])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4aaf1608",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running ingestion pipeline...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d295ff9adf64ab6887b3da9763d7cb5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Parsing nodes:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e507d2200b42471c88b6818167b39228",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating embeddings:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Pipeline complete!\n",
      "  Processed 3 documents\n",
      "  Generated 3 nodes\n",
      "  Nodes have embeddings: True\n"
     ]
    }
   ],
   "source": [
    "# Build ingestion pipeline\n",
    "pipeline = IngestionPipeline(\n",
    "    transformations=[\n",
    "        SentenceSplitter(chunk_size=1024, chunk_overlap=200),\n",
    "        Settings.embed_model,  # Generate embeddings\n",
    "    ],\n",
    ")\n",
    "\n",
    "print(\"Running ingestion pipeline...\")\n",
    "nodes = pipeline.run(documents=sample_papers, show_progress=True)\n",
    "\n",
    "print(f\"\\n✅ Pipeline complete!\")\n",
    "print(f\"  Processed {len(sample_papers)} documents\")\n",
    "print(f\"  Generated {len(nodes)} nodes\")\n",
    "print(f\"  Nodes have embeddings: {nodes[0].embedding is not None}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "094ea949",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Index created from processed nodes\n",
      "  Total nodes indexed: 3\n"
     ]
    }
   ],
   "source": [
    "# Create index from our processed nodes\n",
    "index = VectorStoreIndex(nodes=nodes)\n",
    "\n",
    "query_engine = index.as_query_engine(\n",
    "    similarity_top_k=3,\n",
    "    response_mode=\"compact\"\n",
    ")\n",
    "\n",
    "print(\"✅ Index created from processed nodes\")\n",
    "print(f\"  Total nodes indexed: {len(nodes)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4daeaff9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: What is the Transformer architecture?\n",
      "\n",
      "Response:\n",
      "The Transformer architecture is a neural network design that relies entirely on attention mechanisms to process and transduce sequences. It eliminates the need for recurrence (e.g., RNNs) or convolutional layers by using self-attention to compute representations of input and output sequences. This approach enables parallel processing and captures dependencies between elements regardless of their positional distance, offering a simpler and more efficient alternative to traditional sequence modeling methods.\n",
      "\n",
      "================================================================================\n",
      "\n",
      "Retrieved Sources:\n",
      "\n",
      "Source 1:\n",
      "  Score: 0.8455\n",
      "  Title: Attention Is All You Need\n",
      "  Year: 2017\n",
      "  Category: transformers\n",
      "  Text preview: Title: Attention Is All You Need\n",
      "        Authors: Vaswani et al.\n",
      "        Year: 2017\n",
      "\n",
      "        Abstract: The dominant sequence transduction models are b...\n",
      "\n",
      "Source 2:\n",
      "  Score: 0.7982\n",
      "  Title: BERT\n",
      "  Year: 2019\n",
      "  Category: language_models\n",
      "  Text preview: Title: BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\n",
      "        Authors: Devlin et al.\n",
      "        Year: 2019\n",
      "\n",
      "        Abs...\n",
      "\n",
      "Source 3:\n",
      "  Score: 0.7749\n",
      "  Title: RAG\n",
      "  Year: 2020\n",
      "  Category: rag\n",
      "  Text preview: Title: Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks\n",
      "        Authors: Lewis et al.\n",
      "        Year: 2020\n",
      "\n",
      "        Abstract: Large pre-...\n"
     ]
    }
   ],
   "source": [
    "# Query about transformers\n",
    "query = \"What is the Transformer architecture?\"\n",
    "response = query_engine.query(query)\n",
    "clearResponse = response.response\n",
    "\n",
    "print(f\"Query: {query}\\n\")\n",
    "print(\"Response:\")\n",
    "print(groqLlmResponse(clearResponse))\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "\n",
    "# Examine retrieved sources\n",
    "print(\"\\nRetrieved Sources:\")\n",
    "for i, source_node in enumerate(response.source_nodes, 1):\n",
    "    print(f\"\\nSource {i}:\")\n",
    "    print(f\"  Score: {source_node.score:.4f}\")\n",
    "    print(f\"  Title: {source_node.metadata.get('title', 'N/A')}\")\n",
    "    print(f\"  Year: {source_node.metadata.get('year', 'N/A')}\")\n",
    "    print(f\"  Category: {source_node.metadata.get('category', 'N/A')}\")\n",
    "    print(f\"  Text preview: {source_node.text[:150]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "dbfe3f00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: Explain retrieval-augmented generation\n",
      "\n",
      "Response:\n",
      "Retrieval-augmented generation (RAG) is a framework that enhances the capabilities of language models by integrating external knowledge sources during the generation process. It combines two key components: a parametric model (a large language model) and a non-parametric memory (such as a database or knowledge repository). The parametric model generates text based on its training, while the non-parametric memory provides access to up-to-date or task-specific information that may not be fully captured in the model’s internal parameters. \n",
      "\n",
      "When processing a query, the system first retrieves relevant information from the external memory using techniques like dense vector indexing. This retrieved data is then combined with the model’s own knowledge to produce more accurate, contextually grounded, and interpretable outputs. This approach addresses limitations in traditional models’ ability to dynamically access or update factual knowledge, enabling better performance on tasks requiring precise or current information. Additionally, the modular design allows for transparency, as the retrieved sources can be reviewed to validate the reasoning process.\n",
      "\n",
      "================================================================================\n",
      "\n",
      "Top Source:\n",
      "  Title: RAG\n",
      "  Authors: Lewis et al.\n",
      "  Citations: 3500\n"
     ]
    }
   ],
   "source": [
    "# Query about RAG\n",
    "query2 = \"Explain retrieval-augmented generation\"\n",
    "response2 = query_engine.query(query2)\n",
    "clearResponse2 = response2.response\n",
    "\n",
    "print(f\"Query: {query2}\\n\")\n",
    "print(\"Response:\")\n",
    "print(groqLlmResponse(clearResponse2))\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "\n",
    "print(\"\\nTop Source:\")\n",
    "top_source = response2.source_nodes[0]\n",
    "print(f\"  Title: {top_source.metadata.get('title')}\")\n",
    "print(f\"  Authors: {top_source.metadata.get('authors')}\")\n",
    "print(f\"  Citations: {top_source.metadata.get('citations')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4bbce411",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Chunk Size Impact on Retrieval:\n",
      " Chunk Size  Num Nodes Top Score  Response Len\n",
      "        256          3    0.8143          2356\n",
      "        512          3    0.8143          2953\n",
      "       1024          3    0.8143          3019\n",
      "       2048          3    0.8143          3531\n"
     ]
    }
   ],
   "source": [
    "# Test different chunk sizes\n",
    "chunk_sizes = [256, 512, 1024, 2048]\n",
    "test_query = \"What are the benefits of attention mechanisms?\"\n",
    "\n",
    "results = []\n",
    "\n",
    "for chunk_size in chunk_sizes:\n",
    "    # Create splitter\n",
    "    splitter = SentenceSplitter(\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=int(chunk_size * 0.2)  # 20% overlap\n",
    "    )\n",
    "    \n",
    "    # Process and index\n",
    "    temp_nodes = splitter.get_nodes_from_documents(sample_papers)\n",
    "    temp_index = VectorStoreIndex.from_documents(\n",
    "        sample_papers,\n",
    "        transformations=[splitter],\n",
    "        show_progress=False\n",
    "    )\n",
    "    \n",
    "    # Query\n",
    "    temp_engine = temp_index.as_query_engine(similarity_top_k=2)\n",
    "    temp_response = temp_engine.query(test_query)\n",
    "    \n",
    "    results.append({\n",
    "        \"Chunk Size\": chunk_size,\n",
    "        \"Num Nodes\": len(temp_nodes),\n",
    "        \"Top Score\": f\"{temp_response.source_nodes[0].score:.4f}\",\n",
    "        \"Response Len\": len(str(temp_response)),\n",
    "    })\n",
    "\n",
    "df_results = pd.DataFrame(results)\n",
    "print(\"\\nChunk Size Impact on Retrieval:\")\n",
    "print(df_results.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caf0b408",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".llama",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
