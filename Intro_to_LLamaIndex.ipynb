{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "297367be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Imports successful!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\shara\\OneDrive\\Documents\\Live Courses Krish Naik\\R_A_G\\LLamaIndex\\.llama\\Lib\\site-packages\\llama_index\\embeddings\\gemini\\base.py:7: FutureWarning: \n",
      "\n",
      "All support for the `google.generativeai` package has ended. It will no longer be receiving \n",
      "updates or bug fixes. Please switch to the `google.genai` package as soon as possible.\n",
      "See README for more details:\n",
      "\n",
      "https://github.com/google-gemini/deprecated-generative-ai-python/blob/main/README.md\n",
      "\n",
      "  import google.generativeai as gemini\n"
     ]
    }
   ],
   "source": [
    "# Core LlamaIndex\n",
    "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader, Settings, Document\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "\n",
    "# LLM Integration\n",
    "from llama_index.llms.groq import Groq\n",
    "\n",
    "# Embedding Integration\n",
    "from llama_index.embeddings.gemini import GeminiEmbedding\n",
    "\n",
    "# Utilities\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"✅ Imports successful!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bfca100e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All is okay\n"
     ]
    }
   ],
   "source": [
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Verify OpenAI API key is set\n",
    "if not os.getenv(\"GROQ_API_KEY\"):\n",
    "    print(\"Groq API key not found\")\n",
    "    \n",
    "if not os.getenv(\"GOOGLE_API_KEY\"):\n",
    "    print(\"GOOGLE_API_KEY not found\")\n",
    "    \n",
    "print(\"All is okay\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ade543af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Global Settings configured successfully!\n",
      "   LLM: Qwen/Qwen3-32B\n",
      "   Embedding: models/gemini-embedding-001\n",
      "   Chunk size: 1024 tokens\n",
      "   Chunk overlap: 200 tokens\n"
     ]
    }
   ],
   "source": [
    "# Configure LLM\n",
    "Settings.llm = Groq(model=\"Qwen/Qwen3-32B\",temperature=0.1)\n",
    "\n",
    "# Configure Embedding Model\n",
    "Settings.embed_model = GeminiEmbedding(\n",
    "    model_name=\"models/gemini-embedding-001\",title=\"this is a document\",\n",
    ")\n",
    "\n",
    "# Configure Text Chunking\n",
    "Settings.chunk_size = 1024           # Tokens per chunk (typical: 512-1024)\n",
    "Settings.chunk_overlap = 200         # 20% overlap helps preserve context\n",
    "\n",
    "# Configure Node Parser\n",
    "Settings.node_parser = SentenceSplitter(\n",
    "    chunk_size=Settings.chunk_size,\n",
    "    chunk_overlap=Settings.chunk_overlap,\n",
    ")\n",
    "\n",
    "print(\"✅ Global Settings configured successfully!\")\n",
    "print(f\"   LLM: {Settings.llm.model}\")\n",
    "print(f\"   Embedding: {Settings.embed_model.model_name}\")\n",
    "print(f\"   Chunk size: {Settings.chunk_size} tokens\")\n",
    "print(f\"   Chunk overlap: {Settings.chunk_overlap} tokens\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "198f6fd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Created 3 sample documents\n",
      "   Total characters: 1169\n"
     ]
    }
   ],
   "source": [
    "# Create sample documents (in practice, load from files)\n",
    "documents = [\n",
    "    Document(\n",
    "        text=\"\"\"\n",
    "        LlamaIndex is a data framework for large language models (LLMs). \n",
    "        It provides tools to ingest, structure, and access private or domain-specific data.\n",
    "        LlamaIndex was created to solve the problem of connecting LLMs to external data sources.\n",
    "        The framework supports various data sources including PDFs, databases, APIs, and web pages.\n",
    "        \"\"\",\n",
    "        metadata={\"source\": \"intro\", \"category\": \"overview\"}\n",
    "    ),\n",
    "    Document(\n",
    "        text=\"\"\"\n",
    "        Vector embeddings are numerical representations of text that capture semantic meaning.\n",
    "        In LlamaIndex, embeddings enable semantic search - finding relevant content based on meaning,\n",
    "        not just keyword matching. The default embedding model is OpenAI's text-embedding-3-small,\n",
    "        which produces 1536-dimensional vectors. Other models like all-MiniLM-L6-v2 produce 384 dimensions.\n",
    "        \"\"\",\n",
    "        metadata={\"source\": \"embeddings\", \"category\": \"technical\"}\n",
    "    ),\n",
    "    Document(\n",
    "        text=\"\"\"\n",
    "        The VectorStoreIndex is the most common index type in LlamaIndex. It stores document embeddings\n",
    "        in a vector database and performs similarity search during queries. When you query the index,\n",
    "        it retrieves the most semantically similar chunks and passes them to the LLM as context.\n",
    "        This is the foundation of Retrieval-Augmented Generation (RAG).\n",
    "        \"\"\",\n",
    "        metadata={\"source\": \"vector_index\", \"category\": \"technical\"}\n",
    "    ),\n",
    "]\n",
    "\n",
    "print(f\"✅ Created {len(documents)} sample documents\")\n",
    "print(f\"   Total characters: {sum(len(doc.text) for doc in documents)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "931cc369",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating VectorStoreIndex...\n",
      "This will:\n",
      "  1. Chunk documents into nodes\n",
      "  2. Generate embeddings for each node\n",
      "  3. Store in in-memory vector store\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c10ebfbc40c4c009701a4d8e1c5603d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Parsing nodes:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1548d321ea844a168cfac0c72ba7a4ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating embeddings:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Index created successfully!\n"
     ]
    }
   ],
   "source": [
    "# Create index from documents\n",
    "print(\"Creating VectorStoreIndex...\")\n",
    "print(\"This will:\")\n",
    "print(\"  1. Chunk documents into nodes\")\n",
    "print(\"  2. Generate embeddings for each node\")\n",
    "print(\"  3. Store in in-memory vector store\\n\")\n",
    "\n",
    "index = VectorStoreIndex.from_documents(\n",
    "    documents,\n",
    "    show_progress=True,  # Display progress bar\n",
    ")\n",
    "\n",
    "print(\"\\n✅ Index created successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8a35e286",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Query engine created!\n",
      "   Top-K: 2\n",
      "   Response mode: compact\n"
     ]
    }
   ],
   "source": [
    "# Create query engine from index\n",
    "query_engine = index.as_query_engine(\n",
    "    similarity_top_k=2,  # Retrieve top 2 most similar chunks\n",
    "    response_mode=\"compact\",  # Compact response synthesis\n",
    ")\n",
    "\n",
    "print(\"✅ Query engine created!\")\n",
    "print(f\"   Top-K: {2}\")\n",
    "print(f\"   Response mode: compact\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "891ff652",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def groqLlmResponse(response):\n",
    "    return  re.sub(r\"<think>.*?</think>\", \"\", response, flags=re.DOTALL).strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7a217b4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: What is LlamaIndex used for?\n",
      "\n",
      "Response:\n",
      "--------------------------------------------------------------------------------\n",
      "LlamaIndex is designed to bridge large language models (LLMs) with external data sources by offering tools to ingest, organize, and retrieve private or specialized data. It enables LLMs to access structured information from diverse sources like documents, databases, and APIs. A key application is its support for Retrieval-Augmented Generation (RAG), where it uses vector-based similarity searches to provide contextually relevant data to the model during queries, enhancing the accuracy and relevance of responses.\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Query the index\n",
    " \n",
    "query = \"What is LlamaIndex used for?\"\n",
    "print(f\"Query: {query}\\n\")\n",
    "\n",
    "response_obj = query_engine.query(query)\n",
    "response = response_obj.response\n",
    "response = groqLlmResponse(response)\n",
    "\n",
    "print(\"Response:\")\n",
    "print(\"-\" * 80)\n",
    "print(response)\n",
    "print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "58ea81e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of source nodes: 2\n",
      "\n",
      "Source Node 1:\n",
      "  Score: 0.8994\n",
      "  Metadata: {'source': 'intro', 'category': 'overview'}\n",
      "  Text (first 200 chars): LlamaIndex is a data framework for large language models (LLMs). \n",
      "        It provides tools to ingest, structure, and access private or domain-specific data.\n",
      "        LlamaIndex was created to solve th...\n",
      "\n",
      "Source Node 2:\n",
      "  Score: 0.8580\n",
      "  Metadata: {'source': 'vector_index', 'category': 'technical'}\n",
      "  Text (first 200 chars): The VectorStoreIndex is the most common index type in LlamaIndex. It stores document embeddings\n",
      "        in a vector database and performs similarity search during queries. When you query the index,\n",
      "  ...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Inspect source nodes\n",
    "print(f\"Number of source nodes: {len(response_obj.source_nodes)}\\n\")\n",
    "\n",
    "for i, node in enumerate(response_obj.source_nodes, 1):\n",
    "    print(f\"Source Node {i}:\")\n",
    "    print(f\"  Score: {node.score:.4f}\")  # Similarity score (0-1)\n",
    "    print(f\"  Metadata: {node.metadata}\")\n",
    "    print(f\"  Text (first 200 chars): {node.text[:200]}...\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "02467b40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: How do embeddings work in LlamaIndex?\n",
      "\n",
      "Response:\n",
      "In LlamaIndex, embeddings convert text into numerical vectors that encode semantic meaning, enabling systems to understand and compare the contextual relationships between pieces of text. These vectors are generated using models like OpenAI's text-embedding-3-small (producing 1536-dimensional vectors) or alternatives such as all-MiniLM-L6-v2 (384-dimensional vectors). \n",
      "\n",
      "When documents are processed, their embeddings are stored in a vector database as part of the VectorStoreIndex. During a query, the system calculates the similarity between the query's embedding and stored document embeddings, retrieving the most semantically relevant chunks. These retrieved results are then provided as context to a language model to generate responses, forming the core mechanism of Retrieval-Augmented Generation (RAG). This approach ensures searches are based on meaning rather than exact keyword matches.\n",
      "\n",
      "Top retrieved source:\n",
      "  Category: technical\n",
      "  Score: 0.8925\n"
     ]
    }
   ],
   "source": [
    "query1 = \"How do embeddings work in LlamaIndex?\"\n",
    "response1 = query_engine.query(query1)\n",
    "clearResponse1 = response1.response\n",
    "\n",
    "print(f\"Query: {query1}\\n\")\n",
    "print(\"Response:\")\n",
    "print(groqLlmResponse(clearResponse1))\n",
    "print(\"\\nTop retrieved source:\")\n",
    "print(f\"  Category: {response1.source_nodes[0].metadata.get('category')}\")\n",
    "print(f\"  Score: {response1.source_nodes[0].score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "82a29c20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: What is Retrieval-Augmented Generation?\n",
      "\n",
      "Response:\n",
      "Retrieval-Augmented Generation (RAG) is a method that enhances the responses of large language models by integrating externally retrieved information. It works by first identifying and extracting relevant data from external sources based on the input query, then using that retrieved information as contextual input for the language model. This approach improves the accuracy and relevance of generated outputs by grounding them in domain-specific or up-to-date data, rather than relying solely on the model's internal knowledge.\n"
     ]
    }
   ],
   "source": [
    "query2 = \"What is Retrieval-Augmented Generation?\"\n",
    "response2 = query_engine.query(query2)\n",
    "clearResponse2 = response2.response\n",
    "\n",
    "print(f\"Query: {query2}\\n\")\n",
    "print(\"Response:\")\n",
    "print(groqLlmResponse(clearResponse2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "3f8a3931",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of nodes created: 3\n",
      "\n",
      "Node 1:\n",
      "  ID: a9119f4e-9fc8-4f4d-afc6-b00ee697b65f\n",
      "  Text length: 354 characters\n",
      "  Metadata: {'source': 'intro', 'category': 'overview'}\n",
      "  Relationships: {<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='5bbec56a-f038-4b3d-bbee-8b6484c232bd', node_type=<ObjectType.DOCUMENT: '4'>, metadata={'source': 'intro', 'category': 'overview'}, hash='6b67115e521a90d22245235f7a97b2808451a01b637bc146fc4c6b1b0126d392')}\n",
      "\n",
      "Node 2:\n",
      "  ID: c8246393-f2f5-4c0b-9809-2503b710b920\n",
      "  Text length: 395 characters\n",
      "  Metadata: {'source': 'embeddings', 'category': 'technical'}\n",
      "  Relationships: {<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='a5a8dca4-b54b-49c9-a902-d145d4746991', node_type=<ObjectType.DOCUMENT: '4'>, metadata={'source': 'embeddings', 'category': 'technical'}, hash='c74e00722500b0b510e568d1fd63750c229430501577b8d0a9295eaa48b99fb3')}\n",
      "\n",
      "Node 3:\n",
      "  ID: a21b670b-5551-4e05-b15b-b7e085085e61\n",
      "  Text length: 366 characters\n",
      "  Metadata: {'source': 'vector_index', 'category': 'technical'}\n",
      "  Relationships: {<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='5621ad3a-1997-4111-b2b9-dc90c07df6ba', node_type=<ObjectType.DOCUMENT: '4'>, metadata={'source': 'vector_index', 'category': 'technical'}, hash='e70f293d1f0d5f862e85b3f38f9fcdd444e6238533b3c87fb9d78272b4a683bd')}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Parse documents into nodes manually to understand the flow\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "\n",
    "parser = SentenceSplitter(chunk_size=1024, chunk_overlap=200)\n",
    "nodes = parser.get_nodes_from_documents(documents)\n",
    "\n",
    "print(f\"Number of nodes created: {len(nodes)}\\n\")\n",
    "\n",
    "for i, node in enumerate(nodes, 1):\n",
    "    print(f\"Node {i}:\")\n",
    "    print(f\"  ID: {node.node_id}\")\n",
    "    print(f\"  Text length: {len(node.text)} characters\")\n",
    "    print(f\"  Metadata: {node.metadata}\")\n",
    "    print(f\"  Relationships: {node.relationships}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "0f4a6192",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top-K = 1:\n",
      "  Retrieved 1 nodes\n",
      "  Response length: 1788 characters\n",
      "  First source score: 0.8665\n",
      "\n",
      "Top-K = 2:\n",
      "  Retrieved 2 nodes\n",
      "  Response length: 2994 characters\n",
      "  First source score: 0.8665\n",
      "\n",
      "Top-K = 3:\n",
      "  Retrieved 3 nodes\n",
      "  Response length: 2959 characters\n",
      "  First source score: 0.8665\n"
     ]
    }
   ],
   "source": [
    "# Test with different top_k values\n",
    "test_query = \"Explain vector embeddings\"\n",
    "\n",
    "for k in [1, 2, 3]:\n",
    "    engine = index.as_query_engine(similarity_top_k=k)\n",
    "    response = engine.query(test_query)\n",
    "    \n",
    "    print(f\"\\nTop-K = {k}:\")\n",
    "    print(f\"  Retrieved {len(response.source_nodes)} nodes\")\n",
    "    print(f\"  Response length: {len(str(response))} characters\")\n",
    "    print(f\"  First source score: {response.source_nodes[0].score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "3d715bcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Mode: compact\n",
      "Response: LlamaIndex is designed to bridge large language models (LLMs) with external data sources, offering tools to ingest, structure, and access private or domain-specific data. Key features include support for diverse data formats such as PDFs, databases, APIs, and web pages. It employs a **VectorStoreIndex**, which stores document embeddings in a vector database to enable similarity searches during queries. This process retrieves semantically relevant data chunks, enhancing LLM responses through Retrieval-Augmented Generation (RAG). The framework emphasizes efficient integration of external context into language model workflows.\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Mode: tree_summarize\n",
      "Response: LlamaIndex is designed to enhance the capabilities of large language models (LLMs) by enabling seamless integration with external data sources. Key features include support for diverse data formats such as documents, databases, APIs, and web content, allowing users to ingest and structure domain-specific or private information. A core component is its implementation of Retrieval-Augmented Generation (RAG), which leverages vector-based similarity searches to retrieve contextually relevant data chunks during queries. This process involves storing document embeddings in a vector database, ensuring efficient retrieval of semantically related information to augment LLM responses. These features collectively address the challenge of connecting LLMs to dynamic or specialized datasets.\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Mode: simple_summarize\n",
      "Response: LlamaIndex offers several key features to enhance interactions with large language models (LLMs). It supports seamless integration with diverse data sources such as PDFs, databases, APIs, and web pages, enabling the ingestion and structuring of both private and domain-specific data. A core component is its use of vector-based indexing, which stores document embeddings in a vector database to perform similarity searches during queries. This allows the retrieval of semantically relevant data chunks, which are then used as contextual input for the LLM, forming the basis of Retrieval-Augmented Generation (RAG). Additionally, the framework is designed to efficiently manage and access external data, bridging the gap between LLMs and real-world information sources.\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Test different response modes\n",
    "modes = [\"compact\", \"tree_summarize\", \"simple_summarize\"]\n",
    "test_query = \"What are the key features of LlamaIndex?\"\n",
    "\n",
    "for mode in modes:\n",
    "    engine = index.as_query_engine(\n",
    "        similarity_top_k=2,\n",
    "        response_mode=mode\n",
    "    )\n",
    "    response = engine.query(test_query)\n",
    "    clearResponse = response.response\n",
    "    \n",
    "    print(f\"\\nMode: {mode}\")\n",
    "    print(f\"Response: {groqLlmResponse(clearResponse)}\")\n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f301a18",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".llama",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
